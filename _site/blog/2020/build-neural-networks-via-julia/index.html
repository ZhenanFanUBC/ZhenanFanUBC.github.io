<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Zhenan Fan | Build Neural Networks via Julia</title>
  <meta name="description" content="Personal website.
">

  

  <link rel="shortcut icon" href="http://localhost:4000/assets/img/favicon.ico">

  <link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
  <link rel="canonical" href="http://localhost:4000/blog/2020/build-neural-networks-via-julia/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Zhenan</strong> Fan
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="http://localhost:4000/">about</a>

        <!-- Blog -->
        <a class="page-link" href="http://localhost:4000/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="http://localhost:4000/_pages/publications/">Publications</a>
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="http://localhost:4000/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Build Neural Networks via Julia</h1>
    <p class="post-meta">January 18, 2020</p>
  </header>

  <article class="post-content">
    <p>In this post I want to show how to build a neural network in Julia. Julia is a great programming language for machine learning, which makes buliding neural networks much easier.</p>

<p><a href="https://github.com/FluxML/Flux.jl">Flux</a> is a Julia machine learning package which provides layer-stacking-based interface for buliding simple neural networks such as fully connected neural network and convolutional neural network. Whatâ€™s more, as a differentiable language, Flux is able to take gradients of Julia code, which makes it 
easier to bulid your own more advanced models.</p>

<p>In this post, I will show an example on classifying the handwritten digits in MINST dataset. Most of the material is covered in the <a href="https://fluxml.ai/Flux.jl/v0.10/models/basics/">Flux online library</a>.</p>

<p>Bellow are the packages we are going to use in this example, and I will illustrate them in detail when using them.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="n">Flux</span><span class="x">,</span> <span class="n">Flux</span><span class="o">.</span><span class="n">Data</span><span class="o">.</span><span class="n">MNIST</span><span class="x">,</span> <span class="n">Statistics</span><span class="x">,</span> <span class="n">Printf</span>
<span class="k">using</span> <span class="n">Flux</span><span class="o">:</span> <span class="n">onehotbatch</span><span class="x">,</span> <span class="n">onecold</span><span class="x">,</span> <span class="n">crossentropy</span>
<span class="k">using</span> <span class="n">Base</span><span class="o">.</span><span class="n">Iterators</span><span class="o">:</span> <span class="n">repeated</span>
</code></pre></div></div>

<h3 id="step-1-load-data">Step 1: Load data</h3>
<p>The MINST dataset is contained in <em>Flux.Data.MNIST</em> and we can load the images and labels by the following code:</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># load training images</span>
<span class="n">imgs</span> <span class="o">=</span> <span class="n">MNIST</span><span class="o">.</span><span class="n">images</span><span class="x">(</span><span class="o">:</span><span class="n">train</span><span class="x">)</span>
<span class="c"># load training labels</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">MNIST</span><span class="o">.</span><span class="n">labels</span><span class="x">(</span><span class="o">:</span><span class="n">train</span><span class="x">)</span>
<span class="c"># display the sizes</span>
<span class="n">display</span><span class="x">(</span><span class="n">size</span><span class="x">(</span><span class="n">imgs</span><span class="x">)),</span> <span class="n">display</span><span class="x">(</span><span class="n">size</span><span class="x">(</span><span class="n">labels</span><span class="x">));</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(60000,)
(60000,)
</code></pre></div></div>

<p>As we can see, the MINST dataset contains 60000 images and corresponding labels. We can randomly view some example.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">imgs</span><span class="x">[</span><span class="mi">25</span><span class="x">]</span>
</code></pre></div></div>

<p><img src="/assets/img/post3/post3.svg" alt="Output" /></p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">labels</span><span class="x">[</span><span class="mi">25</span><span class="x">]</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1
</code></pre></div></div>

<p>In order to perform training, we need to do some preprocessing on the data. Formally, we want to store the training images into a matrix $X \in \mathbb{R}^{n \times N}$, where $n$ is the image size and $N$ is the number of training data and we call the julia bulit-in functions <em>reshape</em> and <em>hcat</em> for this purpose. We also want to transform the training labels into their one-hot representations, namely we will create a matrix $Y \in \mathbb{R}^{10 \times N}$, such that each column $Y_i$ is the one-hot representation for $labels[i]$, and the <em>Flux</em> package provides a function called <em>onehotbatch</em> for that.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># pre-processing of training images</span>
<span class="n">trainX</span> <span class="o">=</span> <span class="n">hcat</span><span class="x">(</span><span class="n">float</span><span class="o">.</span><span class="x">(</span><span class="n">reshape</span><span class="o">.</span><span class="x">(</span><span class="n">imgs</span><span class="x">,</span> <span class="o">:</span><span class="x">))</span><span class="o">...</span><span class="x">)</span>
<span class="c"># pre-processing of training labels</span>
<span class="n">trainY</span> <span class="o">=</span> <span class="n">onehotbatch</span><span class="x">(</span><span class="n">labels</span><span class="x">,</span> <span class="mi">0</span><span class="o">:</span><span class="mi">9</span><span class="x">)</span>
<span class="c"># dispaly the size</span>
<span class="n">display</span><span class="x">(</span><span class="n">size</span><span class="x">(</span><span class="n">trainX</span><span class="x">)),</span> <span class="n">display</span><span class="x">(</span><span class="n">size</span><span class="x">(</span><span class="n">trainY</span><span class="x">));</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(784, 60000)
(10, 60000)
</code></pre></div></div>

<p>We do the same pre-processing on validation and test data set.</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># pre-processing of validation data</span>
<span class="n">validationX</span> <span class="o">=</span> <span class="n">hcat</span><span class="x">(</span><span class="n">float</span><span class="o">.</span><span class="x">(</span><span class="n">reshape</span><span class="o">.</span><span class="x">(</span><span class="n">MNIST</span><span class="o">.</span><span class="n">images</span><span class="x">(</span><span class="o">:</span><span class="n">validation</span><span class="x">),</span> <span class="o">:</span><span class="x">))</span><span class="o">...</span><span class="x">)</span>
<span class="n">validationY</span> <span class="o">=</span> <span class="n">onehotbatch</span><span class="x">(</span><span class="n">MNIST</span><span class="o">.</span><span class="n">labels</span><span class="x">(</span><span class="o">:</span><span class="n">validation</span><span class="x">),</span> <span class="mi">0</span><span class="o">:</span><span class="mi">9</span><span class="x">)</span>
<span class="c"># pre-processing of test data</span>
<span class="n">testX</span> <span class="o">=</span> <span class="n">hcat</span><span class="x">(</span><span class="n">float</span><span class="o">.</span><span class="x">(</span><span class="n">reshape</span><span class="o">.</span><span class="x">(</span><span class="n">MNIST</span><span class="o">.</span><span class="n">images</span><span class="x">(</span><span class="o">:</span><span class="n">test</span><span class="x">),</span> <span class="o">:</span><span class="x">))</span><span class="o">...</span><span class="x">)</span>
<span class="n">testY</span> <span class="o">=</span> <span class="n">onehotbatch</span><span class="x">(</span><span class="n">MNIST</span><span class="o">.</span><span class="n">labels</span><span class="x">(</span><span class="o">:</span><span class="n">test</span><span class="x">),</span> <span class="mi">0</span><span class="o">:</span><span class="mi">9</span><span class="x">);</span>
</code></pre></div></div>

<h3 id="step-2-build-neural-network">Step 2: Build neural network</h3>
<p>Flux provides layer-stacking-based interface for buliding simple neural networks. In this example, we show how to build a 2-layer fully connected neural network. <em>Flux</em> package provides a function called <em>Dense</em> for constructing a fully connected layer. More specifically, we can use $Dense(n_1, n_2, \sigma)$ to build a a fully connected layer with input size $n_1$, output size $n_2$ and activation function $\sigma$. <em>Flux</em> package also provides a function called <em>Chain</em>, which allows you combine multiple layers.</p>

<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># bulid two layer neural network</span>
<span class="c"># first layer with activation function relu</span>
<span class="n">layer1</span> <span class="o">=</span> <span class="n">Dense</span><span class="x">(</span><span class="mi">784</span><span class="x">,</span> <span class="mi">32</span><span class="x">,</span> <span class="n">relu</span><span class="x">)</span>
<span class="c"># second layer</span>
<span class="n">layer2</span> <span class="o">=</span> <span class="n">Dense</span><span class="x">(</span><span class="mi">32</span><span class="x">,</span> <span class="mi">10</span><span class="x">)</span>
<span class="c"># construct the model with two layers and softmax function</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Chain</span><span class="x">(</span><span class="n">layer1</span><span class="x">,</span> <span class="n">layer2</span><span class="x">,</span> <span class="n">softmax</span><span class="x">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Chain(Dense(784, 32, relu), Dense(32, 10), softmax)
</code></pre></div></div>

<h3 id="step-3-train-the-neural-network">Step 3: Train the neural network</h3>
<p>To train a model we need several things:</p>
<ul>
  <li>Loss function</li>
  <li>Model</li>
  <li>Data</li>
  <li>Optimizer</li>
  <li>Number of epochs, Stopping Criterion and Logger</li>
</ul>

<h5 id="loss-function">Loss function</h5>
<p>We use cross-entropy as the loss function:</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">y</span><span class="x">)</span> <span class="o">=</span> <span class="n">crossentropy</span><span class="x">(</span><span class="n">model</span><span class="x">(</span><span class="n">x</span><span class="x">),</span> <span class="n">y</span><span class="x">);</span>
</code></pre></div></div>

<h5 id="optimizer">Optimizer</h5>
<p>We set the optimizer to be Adam</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">opt</span> <span class="o">=</span> <span class="n">ADAM</span><span class="x">();</span>
</code></pre></div></div>

<h5 id="number-of-epochs">Number of epochs</h5>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">repeated</span><span class="x">((</span><span class="n">trainX</span><span class="x">,</span> <span class="n">trainY</span><span class="x">),</span><span class="n">num_epochs</span><span class="x">);</span>
</code></pre></div></div>

<h5 id="stopping-criterion-and-logger">Stopping criterion and logger</h5>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># accuracy function</span>
<span class="n">accuracy</span><span class="x">(</span><span class="n">x</span><span class="x">,</span> <span class="n">y</span><span class="x">)</span> <span class="o">=</span> <span class="n">mean</span><span class="x">(</span><span class="n">onecold</span><span class="x">(</span><span class="n">model</span><span class="x">(</span><span class="n">x</span><span class="x">))</span> <span class="o">.==</span> <span class="n">onecold</span><span class="x">(</span><span class="n">y</span><span class="x">))</span> 
<span class="c"># call back function</span>
<span class="n">evalcb</span> <span class="o">=</span> <span class="k">function</span><span class="nf"> </span><span class="o">()</span>
    <span class="nd">@printf</span><span class="x">(</span><span class="s">"training loss=%f, validation accuracy=%f</span><span class="se">\n</span><span class="s">"</span><span class="x">,</span> <span class="n">loss</span><span class="x">(</span><span class="n">trainX</span><span class="x">,</span> <span class="n">trainY</span><span class="x">),</span> <span class="n">accuracy</span><span class="x">(</span><span class="n">validationX</span><span class="x">,</span> <span class="n">validationY</span><span class="x">))</span>
    <span class="n">accuracy</span><span class="x">(</span><span class="n">validationX</span><span class="x">,</span> <span class="n">validationY</span><span class="x">)</span> <span class="o">&gt;</span> <span class="mf">0.9</span> <span class="o">&amp;&amp;</span> <span class="n">Flux</span><span class="o">.</span><span class="n">stop</span><span class="x">()</span>
<span class="k">end</span><span class="x">;</span>
</code></pre></div></div>

<h6 id="start-training">Start training!</h6>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># train the model </span>
<span class="n">Flux</span><span class="o">.</span><span class="n">train!</span><span class="x">(</span><span class="n">loss</span><span class="x">,</span> <span class="n">params</span><span class="x">(</span><span class="n">model</span><span class="x">),</span> <span class="n">data</span><span class="x">,</span> <span class="n">opt</span><span class="x">,</span> <span class="n">cb</span> <span class="o">=</span> <span class="n">evalcb</span><span class="x">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>training loss=2.274012, validation accuracy=0.114500
training loss=2.212757, validation accuracy=0.201000
training loss=2.155514, validation accuracy=0.289900
training loss=2.100332, validation accuracy=0.369900
training loss=2.045855, validation accuracy=0.426000
training loss=1.991192, validation accuracy=0.472200
training loss=1.936068, validation accuracy=0.501000
training loss=1.880539, validation accuracy=0.520200
training loss=1.824895, validation accuracy=0.535300
training loss=1.769496, validation accuracy=0.548500
training loss=1.714780, validation accuracy=0.562000
training loss=1.661142, validation accuracy=0.575000
training loss=1.608866, validation accuracy=0.588600
training loss=1.558143, validation accuracy=0.603600
training loss=1.509105, validation accuracy=0.619600
training loss=1.461786, validation accuracy=0.634200
training loss=1.416273, validation accuracy=0.648600
training loss=1.372575, validation accuracy=0.663700
training loss=1.330685, validation accuracy=0.678000
training loss=1.290625, validation accuracy=0.692600
training loss=1.252400, validation accuracy=0.706000
training loss=1.215952, validation accuracy=0.718200
training loss=1.181204, validation accuracy=0.728900
training loss=1.148056, validation accuracy=0.738600
training loss=1.116362, validation accuracy=0.747900
training loss=1.086002, validation accuracy=0.758900
training loss=1.056879, validation accuracy=0.768000
training loss=1.028928, validation accuracy=0.775800
training loss=1.002120, validation accuracy=0.782900
training loss=0.976431, validation accuracy=0.788300
training loss=0.951837, validation accuracy=0.793400
training loss=0.928310, validation accuracy=0.797300
training loss=0.905823, validation accuracy=0.801500
training loss=0.884321, validation accuracy=0.805500
training loss=0.863768, validation accuracy=0.809200
training loss=0.844120, validation accuracy=0.812100
training loss=0.825350, validation accuracy=0.816400
training loss=0.807434, validation accuracy=0.820800
training loss=0.790344, validation accuracy=0.824900
training loss=0.774048, validation accuracy=0.828800
training loss=0.758511, validation accuracy=0.832100
training loss=0.743701, validation accuracy=0.834900
training loss=0.729580, validation accuracy=0.837200
training loss=0.716109, validation accuracy=0.840700
training loss=0.703246, validation accuracy=0.842700
training loss=0.690952, validation accuracy=0.845200
training loss=0.679184, validation accuracy=0.847600
training loss=0.667905, validation accuracy=0.849300
training loss=0.657084, validation accuracy=0.850800
training loss=0.646692, validation accuracy=0.853000
training loss=0.636705, validation accuracy=0.854600
training loss=0.627104, validation accuracy=0.856300
training loss=0.617868, validation accuracy=0.858000
training loss=0.608979, validation accuracy=0.860300
training loss=0.600417, validation accuracy=0.861400
training loss=0.592167, validation accuracy=0.862600
training loss=0.584213, validation accuracy=0.863700
training loss=0.576541, validation accuracy=0.864600
training loss=0.569139, validation accuracy=0.865300
training loss=0.561991, validation accuracy=0.867000
training loss=0.555090, validation accuracy=0.868400
training loss=0.548419, validation accuracy=0.869700
training loss=0.541974, validation accuracy=0.870400
training loss=0.535748, validation accuracy=0.871600
training loss=0.529726, validation accuracy=0.872500
training loss=0.523896, validation accuracy=0.874400
training loss=0.518253, validation accuracy=0.875800
training loss=0.512788, validation accuracy=0.877200
training loss=0.507496, validation accuracy=0.878000
training loss=0.502365, validation accuracy=0.879300
training loss=0.497391, validation accuracy=0.880800
training loss=0.492567, validation accuracy=0.881600
training loss=0.487889, validation accuracy=0.882500
training loss=0.483348, validation accuracy=0.883700
training loss=0.478941, validation accuracy=0.884100
training loss=0.474661, validation accuracy=0.885200
training loss=0.470502, validation accuracy=0.886000
training loss=0.466459, validation accuracy=0.887400
training loss=0.462528, validation accuracy=0.888200
training loss=0.458703, validation accuracy=0.888400
training loss=0.454980, validation accuracy=0.889000
training loss=0.451356, validation accuracy=0.889900
training loss=0.447825, validation accuracy=0.890400
training loss=0.444386, validation accuracy=0.891200
training loss=0.441035, validation accuracy=0.891600
training loss=0.437769, validation accuracy=0.892100
training loss=0.434585, validation accuracy=0.892800
training loss=0.431479, validation accuracy=0.893500
training loss=0.428449, validation accuracy=0.893800
training loss=0.425493, validation accuracy=0.894400
training loss=0.422608, validation accuracy=0.894900
training loss=0.419792, validation accuracy=0.895900
training loss=0.417041, validation accuracy=0.896500
training loss=0.414354, validation accuracy=0.896800
training loss=0.411728, validation accuracy=0.896900
training loss=0.409160, validation accuracy=0.897100
training loss=0.406649, validation accuracy=0.897700
training loss=0.404194, validation accuracy=0.898900
training loss=0.401792, validation accuracy=0.899400
training loss=0.399442, validation accuracy=0.900000
</code></pre></div></div>

<h3 id="step-4-evaluate-the-result">Step 4: Evaluate the result</h3>
<p>We can evaluate the model by check the accuracy on the test data:</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@show</span><span class="x">(</span><span class="n">accuracy</span><span class="x">(</span><span class="n">testX</span><span class="x">,</span> <span class="n">testY</span><span class="x">));</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>accuracy(testX, testY) = 0.9
</code></pre></div></div>


  </article>

  
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname  = 'zhenanf';
      var disqus_identifier = '/blog/2020/build-neural-networks-via-julia';
      var disqus_title      = "Build Neural Networks via Julia";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2020 Zhenan Fan.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>



    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="http://localhost:4000/assets/js/common.js"></script>


<!-- Load KaTeX -->
<!-- <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
<script src="http://localhost:4000/assets/js/katex.js"></script> -->
<script type="text/javascript" async
   src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          inlineMath: [ ['$', '$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'] ],
          processEscapes: true
        }
      });
</script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.min.css">


<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154621803-2', 'auto');
ga('send', 'pageview');
</script>



  </body>

</html>
