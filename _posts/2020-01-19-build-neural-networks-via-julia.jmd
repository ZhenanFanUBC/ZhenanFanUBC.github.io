# Build Neural Networks via Julia
In this post I want to show how to build a neural network in Julia. Julia is a great programming language for machine learning, which makes buliding neural networks much easier. 
                                                                           
[Flux](https://github.com/FluxML/Flux.jl) is a Julia machine learning package which provides layer-stacking-based interface for buliding simple neural networks such as fully connected neural network and convolutional neural network. What's more, as a differentiable language, Flux is able to take gradients of Julia code, which makes it 
easier to bulid your own more advanced models. 

In this post, I will show an example on classifying the handwritten digits in MINST dataset. Most of the material is covered in the [Flux online library](https://fluxml.ai/Flux.jl/v0.10/models/basics/).

Bellow are the packages we are going to use in this example, and I will illustrate them in detail when using them. 

```julia
using Flux, Flux.Data.MNIST, Statistics, Printf
using Flux: onehotbatch, onecold, crossentropy
using Base.Iterators: repeated
```

### Step 1: Load data
The MINST dataset is contained in *Flux.Data.MNIST* and we can load the images and labels by the following code:

```julia
# load training images
imgs = MNIST.images(:train)
# load training labels
labels = MNIST.labels(:train)
# display the sizes
display(size(imgs)), display(size(labels));
```
As we can see, the MINST dataset contains 60000 images and corresponding labels. We can randomly view some example. 

```julia
imgs[25]
```
```julia
labels[25]
```

In order to perform training, we need to do some preprocessing on the data. Formally, we want to store the training images into a matrix $X \in \mathbb{R}^{n \times N}$, where $n$ is the image size and $N$ is the number of training data and we call the julia bulit-in functions *reshape* and *hcat* for this purpose. We also want to transform the training labels into their one-hot representations, namely we will create a matrix $Y \in \mathbb{R}^{10 \times N}$, such that each column $Y_i$ is the one-hot representation for $labels[i]$, and the *Flux* package provides a function called *onehotbatch* for that. 

```julia
# pre-processing of training images
trainX = hcat(float.(reshape.(imgs, :))...)
# pre-processing of training labels
trainY = onehotbatch(labels, 0:9)
# dispaly the size
display(size(trainX)), display(size(trainY));
```

We do the same pre-processing on validation and test data set. 
```julia
# pre-processing of validation data
validationX = hcat(float.(reshape.(MNIST.images(:validation), :))...)
validationY = onehotbatch(MNIST.labels(:validation), 0:9)
# pre-processing of test data
testX = hcat(float.(reshape.(MNIST.images(:test), :))...)
testY = onehotbatch(MNIST.labels(:test), 0:9);
```

### Step 2: Build neural network
Flux provides layer-stacking-based interface for buliding simple neural networks. In this example, we show how to build a 2-layer fully connected neural network. *Flux* package provides a function called *Dense* for constructing a fully connected layer. More specifically, we can use $Dense(n_1, n_2, \sigma)$ to build a a fully connected layer with input size $n_1$, output size $n_2$ and activation function $\sigma$. *Flux* package also provides a function called *Chain*, which allows you combine multiple layers. 

```julia
# bulid two layer neural network
# first layer with activation function relu
layer1 = Dense(784, 32, relu)
# second layer
layer2 = Dense(32, 10)
# construct the model with two layers and softmax function
model = Chain(layer1, layer2, softmax)
```

### Step 3: Train the neural network
To train a model we need several things:
* Loss function
* Model
* Data
* Optimizer
* Number of epochs, Stopping Criterion and Logger

##### Loss function
We use cross-entropy as the loss function: 
```julia
loss(x, y) = crossentropy(model(x), y);
```
##### Optimizer
We set the optimizer to be Adam
```julia
opt = ADAM();
```

##### Number of epochs
```julia
num_epochs = 100
data = repeated((trainX, trainY),num_epochs);
```

##### Stopping criterion and logger
```julia
# accuracy function
accuracy(x, y) = mean(onecold(model(x)) .== onecold(y)) 
# call back function
evalcb = function ()
    @printf("training loss=%f, validation accuracy=%f\n", loss(trainX, trainY), accuracy(validationX, validationY))
    accuracy(validationX, validationY) > 0.9 && Flux.stop()
end;
```

###### Start training!
```julia
# train the model 
Flux.train!(loss, params(model), data, opt, cb = evalcb)
```

### Step 4: Evaluate the result
We can evaluate the model by check the accuracy on the test data:
```julia
@show(accuracy(testX, testY));
```