<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Zhenan Fan | Smoothing Technique for Nonsmooth Optimization I</title>
  <meta name="description" content="Personal website.
">

  

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/blog/2020/smoothing-techique-for-nonsmooth-optimization-I/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Zhenan</strong> Fan
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">about</a>

        <!-- Blog -->
        <a class="page-link" href="/blog/">blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
            <a class="page-link" href="/publications/">Publications</a>
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/main.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Smoothing Technique for Nonsmooth Optimization I</h1>
    <p class="post-meta">April 18, 2020</p>
  </header>

  <article class="post-content">
    <p>Solving a nonsmooth optimization problem via solving a sequence of approximating smooth problem is a useful and widely used technique in optimization. Consider the first order methods in optimization, if the objective is differentiable, then the normal gradient descent can achieve $\epsilon$ accuracy in function value in $\mathcal{O}(1/\epsilon)$ iterations and those accelerated gradient methods can achieve $\epsilon$ accuracy in $\mathcal{O}(1/\sqrt{\epsilon})$ iterations. However, if the objective is non-differentiable, then the subgradient descent and bundle methods need $\mathcal{O}(1/\epsilon^2)$ iterations to achieve $\epsilon$ accuracy. This illustrates why we can benefit from proper smoothing technique.</p>

<p>In this post, I will discuss the deterministic smoothing techniques for nonsmooth optimization and the randomized ones will be introduced in the next post. Most of the theoretical results here can be found in <sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>, <sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup> and <sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup>.</p>

<h2 id="smooth-approximation">Smooth Approximation</h2>
<p>Throughout this post, we consider $f:\mathbb{R}^n \to \mathbb{R}$ to be a convex but not always differentiable function. We begin by defining the concept of a smooth approximation.</p>

<p><strong>Definition</strong> Let $f_\mu : \mathbb{R}^n \to \mathbb{R}$ be a convex funtion. Then we say $f_\mu$ is a $\mu$-smooth approximation of $f$ with parameter $(\alpha, \beta)$ if</p>

<ul>
  <li>$f_\mu$ is $\frac{\alpha}{\mu}$-smooth</li>
  <li>$f_\mu \leq f \leq f_\mu + \beta\mu$.</li>
</ul>

<p>Here we can view $\mu$ as a tradeoff between approximation accuracy and smothness.</p>

<p><strong>Example (absolute value)</strong>
Consider $f(x)=|x|$ and define the smooth approximation $f_\mu$ as</p>

<div>
$$
f_\mu(x) = 
\begin{cases}
\frac{x^2}{2\mu}, \quad\quad\enspace\text{if}\quad |x| \leq \mu \\
|x| - \frac{\mu}{2}, \quad\text{otherwise}.
\end{cases}
$$
</div>

<p>Note that $f_\mu$ is known to be the Huber function. It is then easy to see that $f_\mu$ is $\frac{1}{\mu}$-smooth and satisfies</p>

<script type="math/tex; mode=display">f_\mu(x) \leq f(x) \leq f_\mu(x) + \frac{\mu}{2}.</script>

<p>Therefore, $f_\mu$ is a $\mu$-smooth approximation of $f$ with parameter $(1,\frac{1}{2})$. $\square$</p>

<p><strong>Example (max)</strong>
Consider $f(x) = \max{x_1, \dots, x_n}$ and define the smooth approximation $f_\mu$ as</p>

<script type="math/tex; mode=display">f_\mu(x) = \mu\log\left(\sum_{i = 1}^n \exp(\frac{x_i}{\mu})\right) - \mu\log(n).</script>

<p>Note that $f_\mu$ is known to be the softmax function. It is not hard to check that $f_\mu$ is $\frac{1}{\mu}$-smooth and satisfies</p>

<script type="math/tex; mode=display">f_\mu(x) \leq f(x) \leq f_\mu(x) + \mu\log(n).</script>

<p>Therefore, $f_\mu$ is a $\mu$-smooth approximation of $f$ with parameter $(1,
 \log(n))$. $\square$</p>

<p>Next, I am going to introduce two widely used smoothing techniques.</p>

<h3 id="moreau-proximal-smoothing">Moreau proximal smoothing</h3>
<p>Moreau proximal smoothing is a widely used smoothing technique in the Euclidean setting. Given a convex function $f:\mathbb{R}^n \to \mathbb{R}$. The Moreau proximal approximation for $f$ is given by</p>

<script type="math/tex; mode=display">f_\mu(x) = \inf_{u \in \mathbb{R}^n} \left\{f(u) + \frac{1}{2\mu}\|x - u\|^2\right\}.</script>

<p>It has been shown by Moreau that $f_\mu$ is $\frac{1}{\mu}$-smooth. Let $u_x$ denote the unique point that achieves the above infimum, which is also known as the proximal point, then the gradient of $f_\mu$ at $x$ is given by</p>

<script type="math/tex; mode=display">\nabla f_\mu(x) = \frac{1}{\mu}(x - u_x).</script>

<p>Moreover, when $f$ is $L$-Lipschitz, then</p>

<script type="math/tex; mode=display">f_\mu(x) \leq f(x) \leq f_\mu(x) + \frac{L^2}{2}\mu.</script>

<p>In other words, when $f$ is $L$-Lipschitz, then $f_\mu$ is a $\mu$-smooth approximation of $f$ with parameter $(1,\frac{L^2}{2})$. It is also worth noting that $f$ and $f_\mu$ have the same set of minimizers, and therefore minimizing $f$ and $f_\mu$ are equivalent.</p>

<h3 id="nesterov-smoothing">Nesterov smoothing</h3>
<p>Nesterovâ€™s smoothing is a smoothing technique for a class of nonsmooth functions:</p>

<script type="math/tex; mode=display">f(x) = \max\{\langle u, Ax\rangle - \phi(u) \mid u \in \mathcal{C}\},</script>

<p>where  $\phi:\mathbb{R}^m\to\mathbb{R}$ is a continuous convex function, $A:\mathbb{R}^n \to \mathbb{R}^m$ is a linear operator and $\mathcal{C}$ is a convex compact set in $\mathbb{R}^m$. The smooth approximation is given by</p>

<script type="math/tex; mode=display">f_\mu(x) = \max\{\langle u, Ax\rangle - \phi(u) - \mu d(u): u \in \mathcal{C}\},</script>

<p>where $d$ is a $\sigma$-strongly convex function on $\mathcal{C}$. It has been shown by Nestrov that $f_\mu$ is $\frac{|A|^2}{\sigma\mu}$-smooth with gradient</p>

<script type="math/tex; mode=display">\nabla f_\mu(x) = A^*u_x,</script>

<p>where $u_x$ is the unique maximizer in the definition of $f_\mu(x)$.</p>

<h2 id="infimal-convolution-smoothing-technique">Infimal convolution smoothing technique</h2>
<p>Beck and Teboulle propose a smoothing technique based on infimal convolution, which can be seen as a generalized framework for the above two discussed smoothing techniques. Let $f:\mathbb{R}^n \to \mathbb{R}$ be a closed and convex function. Then $f$ can be expressed as</p>

<script type="math/tex; mode=display">f(x) = \sup_{u \in \mathbb{R}^n}\enspace \langle u, x\rangle - f^*(u),</script>

<p>where $f^*$ is the conjugate function of $f$. Then we can build a smooth approximation of $f$ by adding a strongly convex component to its conjugate, namely,</p>

<script type="math/tex; mode=display">f_\mu(x) = (f^* + \mu d)^*(x) = \sup_{u \in \mathbb{R}^n}\enspace \langle u, x\rangle - f^*(u) - \mu d(u),</script>

<p>where $d:\mathbb{R}^n\to\mathbb{R}$ is a nonnegative, continuous and 1-strongly convex function. It follows that $f^* + \mu d$ is $\mu$-strongly convex, and therefore $f_\mu$ is $1/\mu$-smooth. It can also be shown that</p>

<script type="math/tex; mode=display">f_\mu(x) \leq f(x) \leq f_\mu(x) + \mu D,</script>

<p>where $D = \sup_x d(x)$. Therefore, $f_\mu$ is a $1/\mu$-smooth approximation of $f$ with parameters $(1, D)$.</p>

<h2 id="complexity-improvement">Complexity Improvement</h2>
<p>Consider the minimization problem:</p>

<script type="math/tex; mode=display">\min_{x \in \mathbb{R}^n} \enspace f(x),</script>

<p>where $f$ is a non-smooth convex funtion. Let $x^*$ denote the optimizer for $f$. Let $f_\mu$ be a $1/\mu$-smooth approximation of $f$ with parameters $(\alpha, \beta)$. Now we consider solving the following problem:</p>

<script type="math/tex; mode=display">\min_{x \in \mathbb{R}^n} \enspace f_\mu(x).</script>

<p>Let $x_\mu^*$ denote the optimizer for $f_\mu$. It is widely known that to attain $\epsilon$-accuracy for minimizing a $L$-smooth convex function by accelerated gradient descent, one needs $\mathcal{O}\left(\sqrt{\frac{L}{\epsilon}}\right)$ iterations. Now we know that $f_\mu$ is $\frac{\alpha}{\mu}$-smooth, in order to attain $\frac{\epsilon}{2}$-accuracy, we need $\mathcal{O}\left(\sqrt{\frac{\alpha}{\mu\epsilon}}\right)$ iteration. Let $x^t$ denote the output. Finally, we set $\mu$ such that $\beta\mu = \frac{\epsilon}{2}$. Then we have</p>

<div>
$$
\begin{align}
f(x^t) - f(x^*) &amp;\leq f(x^t) - f_\mu(x^*)
\\&amp;\leq f(x^t) - f_\mu(x_\mu^*)
\\&amp;= \left(f(x^t) - f_\mu(x^t)\right) + \left(f_\mu(x^t) - f_\mu(x_\mu^*)\right)
\\&amp;\leq \frac{\epsilon}{2} + \frac{\epsilon}{2}
\\&amp;= \epsilon.
\end{align}
$$ 
</div>

<p>The final iteration complexity is given by</p>

<script type="math/tex; mode=display">\mathcal{O}\left(\sqrt{\frac{\alpha}{\mu\epsilon}}\right) = \mathcal{O}\left(\frac{\sqrt{\alpha\beta}}{\epsilon}\right).</script>

<h4 id="references">References</h4>
<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Nesterov, Yu. Smooth minimization of non-smooth functions. Mathematical programming 103.1, 2005.Â <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Beck, Amir, and Marc Teboulle. Smoothing and first order methods: A unified framework. SIAM Journal on Optimization 22.2, 2012.Â <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Chen, Yuxin. Smoothing for nonsmooth optimization. Lecture Notes for ELE 522: Large-Scale Optimization for Data Science, Princeton University, Fall 2019.Â <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </article>

  
    <div id="disqus_thread"></div>
    <script type="text/javascript">
      var disqus_shortname  = 'zhenanf';
      var disqus_identifier = '/blog/2020/smoothing-techique-for-nonsmooth-optimization-I';
      var disqus_title      = "Smoothing Technique for Nonsmooth Optimization I";
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2020 Zhenan Fan.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
  </div>

</footer>



    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


<!-- Load KaTeX -->
<!-- <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js"></script>
<script src="/assets/js/katex.js"></script> -->
<script type="text/javascript" async
   src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
          inlineMath: [ ['$', '$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'] ],
          processEscapes: true
        }
      });
</script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">


<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154621803-2', 'auto');
ga('send', 'pageview');
</script>



  </body>

</html>
